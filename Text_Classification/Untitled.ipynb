{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/home/piedziak/anaconda3/lib/python3.6/site-packages/theano/configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!:</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember,</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Buyer</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Glorious</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Whispers</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Great</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Great</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Oh</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Awful</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Don't</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fashionable</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jobst</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sizes</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mens</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Delicious</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Another</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>i</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Problem</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Works,</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Disappointed:</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Oh</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Based</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>beware:</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>happy</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>oh</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>The</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>Dummy</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>More</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>Take</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>MAGIC</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>Deliciously</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>Magic:</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>Magic,</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>A</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>great</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>Early</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>The</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>\"He's</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>Amazingly</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>A</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>Frightening</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>classic:</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>Psychological</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>A</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>\"The</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>Mauled</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Sorry</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>A</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Great</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Interesting</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Don't</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Beautiful</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               text       label\n",
       "0           Stuning  __label__2\n",
       "1               The  __label__2\n",
       "2         Amazing!:  __label__2\n",
       "3         Excellent  __label__2\n",
       "4         Remember,  __label__2\n",
       "5                an  __label__2\n",
       "6             Buyer  __label__1\n",
       "7          Glorious  __label__2\n",
       "8                 A  __label__2\n",
       "9          Whispers  __label__2\n",
       "10              The  __label__1\n",
       "11            Great  __label__2\n",
       "12            Great  __label__2\n",
       "13               Oh  __label__1\n",
       "14            Awful  __label__1\n",
       "15            Don't  __label__1\n",
       "16                A  __label__2\n",
       "17      Fashionable  __label__2\n",
       "18            Jobst  __label__2\n",
       "19            sizes  __label__1\n",
       "20             mens  __label__1\n",
       "21        Delicious  __label__2\n",
       "22          Another  __label__1\n",
       "23                A  __label__2\n",
       "24                i  __label__2\n",
       "25          Problem  __label__1\n",
       "26           Works,  __label__1\n",
       "27    Disappointed:  __label__1\n",
       "28               Oh  __label__1\n",
       "29            Based  __label__2\n",
       "...             ...         ...\n",
       "9970        beware:  __label__1\n",
       "9971          happy  __label__1\n",
       "9972             oh  __label__1\n",
       "9973            The  __label__2\n",
       "9974          Dummy  __label__1\n",
       "9975           More  __label__1\n",
       "9976           Take  __label__2\n",
       "9977          MAGIC  __label__2\n",
       "9978    Deliciously  __label__2\n",
       "9979         Magic:  __label__2\n",
       "9980         Magic,  __label__2\n",
       "9981              A  __label__2\n",
       "9982          great  __label__1\n",
       "9983          Early  __label__2\n",
       "9984            The  __label__1\n",
       "9985          \"He's  __label__2\n",
       "9986      Amazingly  __label__2\n",
       "9987              A  __label__2\n",
       "9988    Frightening  __label__2\n",
       "9989       classic:  __label__2\n",
       "9990  Psychological  __label__2\n",
       "9991              A  __label__2\n",
       "9992           \"The  __label__1\n",
       "9993         Mauled  __label__1\n",
       "9994          Sorry  __label__1\n",
       "9995              A  __label__2\n",
       "9996          Great  __label__2\n",
       "9997    Interesting  __label__1\n",
       "9998          Don't  __label__1\n",
       "9999      Beautiful  __label__2\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(content[1])\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "\n",
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object http://adataanalyst.com/scikit-learn/countvectorizer-sklearn-example/\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/wiki-news-300d-1M.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d20b378ef7e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the pre-trained word-embedding vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/wiki-news-300d-1M.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/wiki-news-300d-1M.vec'"
     ]
    }
   ],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print \"NB, Count Vectors: \", accuracy\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print \"NB, WordLevel TF-IDF: \", accuracy\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print \"NB, N-Gram Vectors: \", accuracy\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print \"NB, CharLevel Vectors: \", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
